---
title: <span style="color:#00008B">DIABETES PREDICTION</span>
author: "**Group Members:** Shraddha Babar-18101B0065, Siddhi Suryavanshi-18101B0071, Tanaya Desai-18101B0059"
output: html_document
---
<hr />
## <span style="color:purple">STEP-1 : Importing Diabetes Data</span>
<hr />
<style>
div.blue pre { background-color:#c7e0ff; }
</style>
<div class='blue'>
```{r}
#####This will import the data
setwd("C:/Users/Shradha/Desktop/SEM-8/R Lab/MiniProject")
diabetes <- read.csv("diabetes.csv")
```
</div>
<style>
div.pink pre { background-color:#fad9d9; }
div.pink pre.r { background-color:#ffc78f; }

</style>
<hr />
## <span style="color:purple">STEP-2 : Exploratory Data Analysis on Diabetes Data</span>
<hr/>
### <span style="color:#1387ed">Head : head()</span>
Head is a function which returns the first 6 observations of the dataset.

<div class='pink'>
```{r}
head(diabetes)
```
</div>
<hr />
### <span style="color:#1387ed">Summary : summary()</span>
Here we are computing the minimum,1st quartile, median, mean,3rd quartile and the maximum for all numeric variables of a dataset at once using summary() function.

<div class='pink'>
```{r}
summary(diabetes)
```
</div>
<hr />
### <span style="color:#1387ed">Structure : str()</span>
The structure() function displays the internal structure of a data object.

<div class='pink'>
```{r}
str(diabetes)
```
</div>
<hr />
### <span style="color:#1387ed">Column Names : colnames()</span>
The colname() function displays the column names available in the dataset.

<div class='pink'>
```{r}
colnames(diabetes)
```
</div>
<hr />
### <span style="color:#1387ed">Minimum : min() & Maximum : max()</span>
Here we will find the minimum and maximum value from Glucose column of diabetes dataset.

<div class='pink'>
```{r}
min_glucose <- min(diabetes$Glucose)
print(paste("Minimum Glucose Value :",min_glucose))

max_glucose <- max(diabetes$Glucose)
print(paste("Maximum Glucose Value :",max_glucose))
```
</div>
<hr />
### <span style="color:#1387ed">Range : range()</span>
Range function gives you the minimum and maximum directly in the form of an object and we need to access it as shown below.

<div class='pink'>
```{r}
range_Glucose <- range(diabetes$Glucose)
print(range_Glucose)
print(paste("Minimum Glucose Value :",range_Glucose[1]))
print(paste("Maximum Glucose Value :",range_Glucose[2]))
```
</div>
<hr />
### <span style="color:#1387ed">Mean : mean()</span>
Mean is calculated by taking the sum of the values and dividing with the number of values in a data series. Here we will find the mean of Glucose column.

<div class='pink'>
```{r}
Mean_Glucose <- mean(diabetes$Glucose)
print(paste("Mean of Glucose :",Mean_Glucose))
```
</div>
<hr />
### <span style="color:#1387ed">Median : median()</span>
The middle most value in a data series is called the median. Let us find this median from Glucose column.

<div class='pink'>
```{r}
Median_Glucose <- median(diabetes$Glucose)
print(paste("Median of Glucose :",Median_Glucose))
```
</div>
<hr />
### <span style="color:#1387ed">Mode : table() & sort()</span>
The mode is the value that has highest number of occurrences in a set of data. There is no function to find the mode of a variable. However, we can easily find it thanks to the functions table() and sort().

Let us find the value that is most repeated in Glucose column.

<div class='pink'>
```{r}
Mode_Glucose <- table(diabetes$Glucose)
sort(Mode_Glucose,decreasing = TRUE)
```
</div>
<hr />
### <span style="color:#1387ed">First & Third Quartile : quantile()</span>
The quantile function divides the data into equal halves, in which the median acts as middle and over that the remaining lower part is lower quartile and upper part is upper quartile.

<div class='pink'>
```{r}
q1 <- quantile(diabetes$Glucose,0.25) # first quartile
print(paste("First Quartile :",q1))
q3 <- quantile(diabetes$Glucose,0.75) # third quartile
print(paste("Third Quartile :",q3))
```
</div>
<hr />
### <span style="color:#1387ed">Interquartile Range : IQR()</span>
The interquartile range (i.e., the difference between the first and third quartile) can be computed with the IQR()function.So, let's find the IQR for Glucose column. Alternatively with the quantile() function (as we already used the quantile function and calculated 1st and 3rd quantile we will directly subtract the values).

<div class='pink'>
```{r}
IQR_Glucose <- IQR(diabetes$Glucose)
print(paste("Interquartile range for Glucose :",IQR_Glucose))
#alternative (refer First & Third Quartile section)
iqr_gluco <- q3 -q1
print(paste("Interquartile range for Glucose :",iqr_gluco))
```
</div>
<hr />
### <span style="color:#1387ed">Standard Deviation : sd() & Variance : var()</span>
Standard Deviation is a measure of the amount of variation in a set of values.
Variance is a measure of how data points differ from the mean.

<div class='pink'>
```{r}
sd_Glucose <- sd(diabetes$Glucose)
print(paste("Standard Deviation for Glucose Column :",sd_Glucose))
var_Glucose <- var(diabetes$Glucose)
print(paste("Variance for Glucose Column :",var_Glucose))
```
</div>
<hr />
## <span style="color:purple">STEP-3 : Prediting Diabetes</span>
<hr />
<div class='blue'>
```{r setup, include=TRUE,message=FALSE}
#DO NOT MODIFY THIS CODE
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) #for data visualization
library(grid) # for grids
library(gridExtra) # for arranging the grids
library(corrplot) # for Correlation plot
library(caret) # for confusion matrix
library(e1071) # for naive bayes
```
</div>
</div>
<style>
div.green pre { background-color:#fdd6ff; }
div.green pre.r { background-color:#d8ffd6; }

</style>
### <span style="color:#1387ed">Plotting Histograms of Numeric Values</span>
<div class='green'>
```{r,fig.align='center',message=FALSE}
p1 <- ggplot(diabetes, aes(x=Pregnancies)) + ggtitle("Number of times pregnant") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 1, colour="black", fill="blue") + ylab("Percentage")
p2 <- ggplot(diabetes, aes(x=Glucose)) + ggtitle("Glucose") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 5, colour="black", fill="orange") + ylab("Percentage")
p3 <- ggplot(diabetes, aes(x=BloodPressure)) + ggtitle("Blood Pressure") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 2, colour="black", fill="green") + ylab("Percentage")
p4 <- ggplot(diabetes, aes(x=SkinThickness)) + ggtitle("Skin Thickness") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 2, colour="black", fill="pink") + ylab("Percentage")
p5 <- ggplot(diabetes, aes(x=Insulin)) + ggtitle("Insulin") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 20, colour="black", fill="red") + ylab("Percentage")
p6 <- ggplot(diabetes, aes(x=BMI)) + ggtitle("Body Mass Index") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 1, colour="black", fill="yellow") + ylab("Percentage")
p7 <- ggplot(diabetes, aes(x=DiabetesPedigreeFunction)) + ggtitle("Diabetes Pedigree Function") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), colour="black", fill="purple") + ylab("Percentage")
p8 <- ggplot(diabetes, aes(x=Age)) + ggtitle("Age") +
  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth=1, colour="black", fill="lightblue") + ylab("Percentage")
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol=2)
grid.rect(width = 1, height = 1, gp = gpar(lwd = 1, col = "black", fill = NA))
```
</div>
#### All the variables seem to have reasonable broad distribution, therefore, will be kept for the regression analysis.
<hr />
### <span style="color:#1387ed">Correlation between Numeric Variables</span>
Here, **sapply()** function will return the columns from the diabetes dataset which have numeric values. **cor()** function will produce correlation matrix of all those numeric columns returned by sapply(). **corrplot()** provides a visual representation of correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.

<div class='green'>
```{r,fig.align='center',message=FALSE}
numeric.var <- sapply(diabetes, is.numeric)
corr.matrix <- cor(diabetes[,numeric.var])
corrplot(corr.matrix, main="\n\nCorrelation Plot for Numerical Variables", order = "hclust", tl.col = "black", tl.srt=45, tl.cex=0.8, cl.cex=0.8)
box(which = "outer", lty = "solid")
```
</div>
#### The numeric variables are almost not correlated.
<hr />
### <span style="color:#1387ed">Correlation between Numeric Variables and Outcomes</span>

<div class='green'>
```{r,fig.align='center',fig.width=10,fig.height=5,message=FALSE}
attach(diabetes)
par(mfrow=c(2,4))
boxplot(Pregnancies~Outcome, main="No. of Pregnancies vs. Diabetes", 
        xlab="Outcome", ylab="Pregnancies",col="red")
boxplot(Glucose~Outcome, main="Glucose vs. Diabetes", 
        xlab="Outcome", ylab="Glucose",col="pink")
boxplot(BloodPressure~Outcome, main="Blood Pressure vs. Diabetes", 
        xlab="Outcome", ylab="Blood Pressure",col="green")
boxplot(SkinThickness~Outcome, main="Skin Thickness vs. Diabetes", 
        xlab="Outcome", ylab="Skin Thickness",col="orange")
boxplot(Insulin~Outcome, main="Insulin vs. Diabetes", 
        xlab="Outcome", ylab="Insulin",col="yellow")
boxplot(BMI~Outcome, main="BMI vs. Diabetes", 
        xlab="Outcome", ylab="BMI",col="purple")
boxplot(DiabetesPedigreeFunction~Outcome, main="Diabetes Pedigree Function vs. Diabetes", xlab="Outcome", ylab="DiabetesPedigreeFunction",col="lightgreen")
boxplot(Age~Outcome, main="Age vs. Diabetes", 
        xlab="Outcome", ylab="Age",col="lightblue")
box(which = "outer", lty = "solid")
```
</div>
#### Blood pressure and skin thickness show little variation with diabetes, they will be excluded from the model. Other variables show more or less correlation with diabetes, so will be kept.
<hr />
### <span style="color:#fc6f03">**Linear Regression**</span>
<div class='green'>
```{r}
diabetes$BloodPressure <- NULL
diabetes$SkinThickness <- NULL
train <- diabetes[1:540,]
test <- diabetes[541:768,]
model <-glm(Outcome ~.,family=binomial(link='logit'),data=train)
summary(model)
```
</div>
#### The top three most relevant features are “Glucose”, “BMI” and “Number of times pregnant” because of the low p-values.

#### “Insulin” and “Age” appear not statistically significant.
<div class='green'>
```{r}
anova(model, test="Chisq")
```
</div>
#### From the table of deviance, we can see that adding insulin and age have little effect on the residual deviance.
<hr />
### <span style="color:#1387ed">Cross Validation</span>
<div class='green'>
```{r}
fitted.results <- predict(model,newdata=test,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
(conf_matrix_logi<-table(fitted.results, test$Outcome))
misClasificError <- mean(fitted.results != test$Outcome)
print(paste('Accuracy',1-misClasificError))
```
</div>
<hr />
### <span style="color:#fc6f03">**Decision Tree**</span>
<div class='green'>
```{r,fig.align='center',fig.width=15,fig.height=10,message=FALSE}
library(rpart)
model2 <- rpart(Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction, data=train,method="class")
plot(model2, uniform=TRUE, 
  	main="Classification Tree for Diabetes")
text(model2, use.n=TRUE, all=TRUE, cex=.8)
box(which = "outer", lty = "solid")
```
</div>
#### This means if a person’s BMI less than 45.4 and his/her Diabetes Pedigree function less than 0.8745, then the person is more likely to have diabetes.
<hr />
### <span style="color:#1387ed">Confusion Table and Accuracy</span>
<div class='green'>
```{r}
treePred <- predict(model2, test, type = 'class')
(conf_matrix_dtree<-table(treePred, test$Outcome))
mean(treePred==test$Outcome)
```
</div>
<hr />
### <span style="color:#fc6f03">**Naive Bayes**</span>
<div class='green'>
```{r}
# creating Naive Bayes model
model_naive <- naiveBayes(Outcome ~., data = train)
```
</div>
<hr />
### <span style="color:#1387ed">Confusion Table and Accuracy</span>
<div class='green'>
```{r}
# predicting target 
toppredict_set <- test[1:6]
dim(toppredict_set)
preds_naive <- predict(model_naive, newdata = toppredict_set)
(conf_matrix_naive <- table(preds_naive, test$Outcome))
mean(preds_naive==test$Outcome)
```
</div>
<hr />
### <span style="color:purple">**Conclusion**</span>
<hr />
#### If we compare accuracy and sensitivity level of our models to see the highest value, we can summarise as followed :
<div class='pink'>
```{r}
confusionMatrix(conf_matrix_logi)
confusionMatrix(conf_matrix_dtree)
confusionMatrix(conf_matrix_naive)
```
</div>
<hr/>
#### In this project, we compared the performance of Linear Regression and Decision Tree algorithms and found that Linear Regression performed better on this standard, unaltered dataset. After, Linear Regression there comes Naive Bayes algorithm with more accuracy tha the Decision Tree. Accuracy given by Linear Regression was 79%, Decision Tree was 74% and Naive Bayes was 78%.
<hr />